<HTML>
<HEAD>
<meta http-equiv="Content-Type" content="text/html; Charset=UTF-8" /> 
<TITLE>Large Language Models - Bachelors Seminar (SS 2024)</TITLE>
</HEAD>
<BODY>
<H2>Large Language Models - Bachelors Seminar (SS 2024)</H2>

<H2>Summary</H2>

<P>Large Language Models (such as GPT2, GPT3, GPT4, RoBERTa, T5) and Intelligent Chatbots (such as ChatGPT, Bard and Claude) are a very timely topic.

<P>Contents:

<P>N-gram language models, neural language modeling, word2vec, RNNs, Transformers, BERT, RLHF, ChatGPT, multilingual alignment, prompting, transfer learning, domain adaptation, linguistic knowledge in large language models

<P>Learning Goals:

<P>The participants will first learn the basics of n-gram language models, neural language modeling, RNNs and Transformers. In the second half of the seminar, participants will present an application of a modern large language model, intelligent chatbot or similar system. This class will involve a large amount of reading on both the basics and advanced topics.

<BR>

<H2>Instructors</H2>

<P><A HREF="..">Alexander Fraser</A> 
<BR>email: contact101@alexanderfraser.de
<P>
<P><A HREF="..">TUM</A>

<p>
<br>

<p>Dr. Marion Di Marco
<br>
<p>TUM
<br>
  
<p>
<br>

<H2>Schedule</H2>

<p>Tuesdays: 16:15-17:45, D.2.12
<br>
<p>
<br>
<p>
<br>Read the materials *before* class!
<br>

<table border="2">
<tr> <td> Date </td> <td> Topic </td> <td> Materials </td></tr>
<tr> <td> April 16th </td> <td> Introduction to the class (AF) and Language for LLMs (MDM) </td> <td> <a href="orientation.pdf">orientation.pdf</a> <br> <a href="01_language.pdf">Linguistic Background for LLMs</td> </tr>
<tr> <td> April 23rd </td> <td> Dan Jurafsky and James H. Martin (2023). Speech and Language Processing (3rd ed. draft), Chapter 3, N-gram Language Models </td>  <td> <A HREF="https://web.stanford.edu/~jurafsky/slp3/3.pdf">article</A> <A HREF="02_ngrams.pdf">lecture</a>  </td>   </tr>
<tr> <td> April 30th </td> <td> Y Bengio, R Ducharme, P Vincent, C Jauvin (2003). A neural probabilistic language model. Journal of Machine Learning Research 3, 1137-1155 </td> <td> <A HREF="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">pdf</A> </td>  </tr>
<tr> <td> May 7th </td> <td> Noah A. Smith (2020). Contextual Word Representations: A Contextual Introduction. arXiv. </td> <td> <A HREF="https://arxiv.org/abs/1902.06006">pdf</A> </td>  </tr>
<tr> <td> May 14th </td> <td> Lena Voita. NLP Course: Neural Language Models, Sequence to Sequence (seq2seq) and Attention. Web Tutorial </td>  <td> <A HREF="https://lena-voita.github.io/nlp_course/language_modeling.html">neural language models</A> <br> <A HREF="https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html">seq2seq and attention</A> </td>   </tr>
<tr> <td> May 28th </td> <td> Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin (2017). Attention Is All You Need. NIPS </td>  <td> <A HREF="https://arxiv.org/abs/1706.03762">paper</A>  </td>   </tr>
<tr> <td> June 4th </td> <td> Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. NAACL-HLT  </td>  <td> <A HREF="https://aclanthology.org/N19-1423">paper</A> <BR> <A HREF="https://aclanthology.org/2023.acl-long.367/">project-paper</a> <br> <A HREF="https://universaldependencies.org/treebanks/en_gum/index.html">project-data</a> </td>   </tr>
<tr> <td> June 11th </td> <td> Long Ouyang, Jeff Wu, et al. (2022). Training language models to follow instructions with human feedback. arXiv. </td>  <td> <a href="https://arxiv.org/abs/2203.02155">paper (InstructGPT) </a> </td>  </tr>
</table> 
<p>
<br>

</BODY>
</HTML>
