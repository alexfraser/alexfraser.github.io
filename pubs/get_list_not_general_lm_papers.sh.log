Q: Is it contextual representation learning?
NO Woller is type-level BLI
UNMT Chron EMNLP 2020 uses a monolingual high-resource LM, which is adjusted to low resource (this is exactly what is in TRR?)
UNMT Chron NAACL 2021 uses type-level to enhance pretrained bilingual for UNMT
NO Chron WMT 2020 unsup Sorbian (includes adapters)
NO Eder ACL 2021 anchoring
NO Fraser WMT 2020 Sorbian tasks
NO Hangya AMTA 2020 compositionality in static
SORTOF Henning Findings EMNLP 2023 is on unbalanced class sup
NMT Lai COLING 2022 Domain robust/adaptability in NMT
NMT Lai Findings EMNLP 2022 multiling multi-domain meta-adapter for NMT
NMT Lai WMT 2021 Large-scale multiling shared task, 30 biling trans systems, data aug, knowledge distillation. "Only" = best?
NO? Libo SPNLP 2022 Neural String Edit Distance (should be resubbed to journal!)
NMT Libo WMT 2020 shared very low transfer
NMT Libo WMT 2021 iterated back translation
NMT Libo WMT 2021 task Sorbian
NO Yi-Chen Liu BIGSURV 2020 translation verification
NO Michel LREC 2020 Hiligaynon embeddings
NMT Riess MTSUMMIT 2021 Sentence weighting for NMT
NO Severini BUCC 2020 BLI word surface sim
NO Severini COLING 2020 BLI + orthography embeddings
NMT Stojanovski Adapt 2021 Zero-resource domains, doc context
NMT Stojanovski COLING 2020 ContraCAT - pronoun translation
NO Tan GSCL 2021 Improved Hiligaynon
NMT Weller-Di ACL 2020 Word formation EN-DE
NMT Weller-Di arxiv 2022 Two strategies morph
NMT Weller-di WMT 2022 findings shared sorbian
NMT Weller-di WMT 2022 morph challenges and pronoun
