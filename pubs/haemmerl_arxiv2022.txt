Do Multilingual Language Models Capture Differing Moral Norms?

Katharina Hämmerl, Björn Deiseroth, Patrick Schramowski, Jindřich Libovický, Alexander Fraser, Kristian Kersting

https://arxiv.org/abs/2203.09904

Massively multilingual sentence representations are trained on large
corpora of uncurated data, with a very imbalanced proportion of
languages included in the training. This may cause the models to grasp
cultural values including moral judgments from the high-resource
languages and impose them on the low-resource languages. The lack of
data in certain languages can also lead to developing random and thus
potentially harmful beliefs. Both these issues can negatively
influence zero-shot cross-lingual model transfer and potentially lead
to harmful outcomes. Therefore, we aim to (1) detect and quantify
these issues by comparing different models in different languages, (2)
develop methods for improving undesirable properties of the
models. Our initial experiments using the multilingual model XLM-R
show that indeed multilingual LMs capture moral norms, even with
potentially higher human-agreement than monolingual ones. However, it
is not yet clear to what extent these moral norms differ between
languages.

