Improving Both Domain Robustness and Domain Adaptability in Machine Translation

Wen Lai, Jindrich LibovickyÃÅ, Alexander Fraser

COLING 2022

We consider two problems of NMT domain adaptation using
meta-learning. First, we want to reach domain robustness, i.e., we
want to reach high quality on both domains seen in the training data
and unseen domains. Second, we want our systems to be adaptive, i.e.,
making it possible to finetune systems with just hundreds of in-domain
parallel sentences. We study the domain adaptability of meta-learning
when improving the domain robustness of the model. In this paper, we
propose a novel approach, RMLNMT (Robust Meta-Learning Framework for
Neural Machine Translation Domain Adaptation), which improves the
robustness of existing meta-learning models. More specifically, we
show how to use a domain classifier in curriculum learning and we
integrate the word-level domain mixing model into the meta-learning
framework with a balanced sampling strategy. Experiments on English to
German and English to Chinese translation show that RMLNMT improves in
terms of both domain robustness and domain adaptability in seen and
unseen domains.
